<h1>Download</h1>
<ol>
  <li>http://ftp.carnet.hr/misc/apache/hadoop/core/hadoop-1.2.1/</li>
</ol>

<h1> Links </h1>
<ol>
  <li>http://www.fer.unizg.hr/_download/repository/lab2%5B8%5D.pdf</li>
  <li>http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</li>
</ol>

<h1>  Transfer files, create *.class files and pack in .jar </h1>
<ol>
  <li><b> Login as hduser </b></li>
  <li><b> Go to: </b> /usr/local/hadoop</li>
  <li> cp /home/damir/Desktop/hadoop-video/VideoCountReduce.java /usr/local/hadoop</li>
  <li> cp /home/damir/Desktop/hadoop-video/VideoCountMap.java /usr/local/hadoop</li>
  <li> cp /home/damir/Desktop/hadoop-video/VideoCount.java /usr/local/hadoop</li>
  <li> javac -classpath /usr/local/hadoop/hadoop-core-1.2.1.jar *.java</li>
  <li> jar -cvf VideoCount.jar -C VideoCount.class VideoCountReduce.class VideoCountMap.class</li>
</ol>


<h1> Generate a fresh list of most visited URL paths </h1>

<ol>
  <li> <b> ssh localhost</b></li>
  <li><b>Start the cluster:</b> /usr/local/hadoop/bin/start-all.sh</li>
  <li><b>Delete old read directory from HDFS</b>  bin/hadoop dfs -rmr /user/hduser/rznu-logs</li>
  <li><b> Delete old output directory from HDFS</b> bin/hadoop dfs -rmr /user/hduser/browser-output</li>
  <li><b>Copy to HDFS:</b> bin/hadoop dfs -copyFromLocal /home/damir/Desktop/rails_projects/rznu-twitter/log/rznu.log /user/hduser/rznu-logs </li>
  <li><b>View files:</b> bin/hadoop dfs -ls /user/hduser </li>
  <li><b>Start MapReduce:</b> bin/hadoop jar VideoCount.jar VideoCount /user/hduser/rznu-logs /user/hduser/browser-output</li>
  <li></b>Delete old file:</b> rm /tmp/rznu-logs-map-reduce/browser-output </li>
  <li><b>Fetch files to the temp folder:</b> bin/hadoop dfs -getmerge /user/hduser/browser-output /tmp/rznu-logs-map-reduce/</li>
  <li><b>Stop the cluster:</b> /usr/local/hadoop/bin/stop-all.sh</li>
</ol>